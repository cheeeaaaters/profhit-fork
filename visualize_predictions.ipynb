{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from hierarchy_data import (\n",
    "    CustomHierarchyData,\n",
    "    normalize_data,\n",
    "    unnormalize_data,\n",
    ")\n",
    "from models.fnpmodels import EmbedMetaAttenSeq, RegressionSepFNP, Corem\n",
    "from utils import lag_dataset_2\n",
    "from models.utils import float_tensor, long_tensor\n",
    "from random_split import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Desktop\\PROFHiT\\PROFHiT\\hierarchy_data\\__init__.py:56: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  shapes = [df[str(date)].shape[0] for date in dates]\n"
     ]
    }
   ],
   "source": [
    "data_obj = CustomHierarchyData()\n",
    "\n",
    "# Let's create dataset\n",
    "full_data = data_obj.data\n",
    "train_data_raw = full_data\n",
    "train_means = np.mean(train_data_raw, axis=1)\n",
    "train_std = np.std(train_data_raw, axis=1)\n",
    "train_data = (train_data_raw - train_means[:, None]) / train_std[:, None]\n",
    "num_stocks = train_data.shape[0]\n",
    "\n",
    "dataset_raw = lag_dataset_2(train_data, data_obj.shapes, 1, 50)\n",
    "\n",
    "class SeqDataset(th.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.R, self.X, self.Y = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.R[idx], self.X[idx], self.Y[idx]\n",
    "\n",
    "dataset = SeqDataset(dataset_raw)\n",
    "train_dataset, val_dataset = random_split(dataset, [0.7, 0.3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "device = th.device('cuda')\n",
    "# Let's create FNP model\n",
    "encoder = EmbedMetaAttenSeq(\n",
    "    dim_seq_in=1,\n",
    "    num_metadata=len(data_obj.idx_dict),\n",
    "    dim_metadata=1,\n",
    "    dim_out=60,\n",
    "    n_layers=2,\n",
    "    bidirectional=True,\n",
    ").to(device)\n",
    "decoder = RegressionSepFNP(\n",
    "    dim_x=60,\n",
    "    dim_y=1,\n",
    "    dim_h=60,\n",
    "    n_layers=3,\n",
    "    dim_u=60,\n",
    "    dim_z=60,\n",
    "    nodes=len(data_obj.idx_dict),\n",
    ").to(device)\n",
    "corem = Corem(nodes=len(data_obj.idx_dict), c=5.0, ).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(th.load('output/custom/encoder_train_32300_0.009501314722001553.pt'))\n",
    "decoder.load_state_dict(th.load('output/custom/decoder_train_32300_0.009501314722001553.pt'))\n",
    "corem.load_state_dict(th.load('output/custom/corem_train_32300_0.009501314722001553.pt'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(258, 421)\n"
     ]
    }
   ],
   "source": [
    "r, x, y = val_dataset[0]\n",
    "print(r.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1876],\n",
      "        [ 5.2453],\n",
      "        [ 0.4134],\n",
      "        [-2.2084],\n",
      "        [ 1.2602],\n",
      "        [ 1.6831],\n",
      "        [-1.6378],\n",
      "        [ 2.7918],\n",
      "        [ 1.7766],\n",
      "        [-1.7858],\n",
      "        [ 2.6688],\n",
      "        [-1.6808],\n",
      "        [-0.4657],\n",
      "        [ 2.8717],\n",
      "        [-0.0562],\n",
      "        [-0.2698],\n",
      "        [-1.2731],\n",
      "        [-2.1122],\n",
      "        [ 4.0665],\n",
      "        [ 1.3399],\n",
      "        [-3.0242],\n",
      "        [-0.8552],\n",
      "        [ 2.0657],\n",
      "        [-1.6593],\n",
      "        [ 1.3300],\n",
      "        [-1.9929],\n",
      "        [ 0.4621],\n",
      "        [ 0.0825],\n",
      "        [-5.1440],\n",
      "        [ 0.8703],\n",
      "        [ 0.5278],\n",
      "        [-0.7531],\n",
      "        [-1.4341],\n",
      "        [-4.4863],\n",
      "        [ 0.2163],\n",
      "        [-1.3403],\n",
      "        [ 1.4094],\n",
      "        [ 1.2761],\n",
      "        [ 3.1337],\n",
      "        [-1.4137],\n",
      "        [ 1.9653],\n",
      "        [ 0.6822],\n",
      "        [-1.5970],\n",
      "        [ 2.9435],\n",
      "        [-3.3739],\n",
      "        [-0.6599],\n",
      "        [-0.5041],\n",
      "        [ 2.1994],\n",
      "        [ 0.9753],\n",
      "        [ 3.0346],\n",
      "        [-2.0694],\n",
      "        [-2.8768],\n",
      "        [ 3.7162],\n",
      "        [ 2.1800],\n",
      "        [-0.1989],\n",
      "        [ 0.9734],\n",
      "        [-4.2025],\n",
      "        [-0.6631],\n",
      "        [-3.7741],\n",
      "        [-0.5377],\n",
      "        [ 0.3760],\n",
      "        [ 0.6752],\n",
      "        [ 0.0658],\n",
      "        [-0.3938],\n",
      "        [ 2.4349],\n",
      "        [ 1.7193],\n",
      "        [ 1.4944],\n",
      "        [ 2.6774],\n",
      "        [ 2.4057],\n",
      "        [ 2.1359],\n",
      "        [ 1.0538],\n",
      "        [-1.3293],\n",
      "        [-0.9713],\n",
      "        [ 1.0214],\n",
      "        [-1.7066],\n",
      "        [ 0.1053],\n",
      "        [ 2.9583],\n",
      "        [-3.7111],\n",
      "        [ 1.8911],\n",
      "        [-2.7960],\n",
      "        [ 2.2698],\n",
      "        [-1.6056],\n",
      "        [-0.7219],\n",
      "        [-0.1272],\n",
      "        [-2.4687],\n",
      "        [ 2.7492],\n",
      "        [ 0.5642],\n",
      "        [ 0.0313],\n",
      "        [ 2.0744],\n",
      "        [-0.0596],\n",
      "        [ 1.9203],\n",
      "        [ 0.0651],\n",
      "        [-2.1984],\n",
      "        [ 0.4591],\n",
      "        [ 0.7010],\n",
      "        [-1.9576],\n",
      "        [ 1.1442],\n",
      "        [ 0.0077],\n",
      "        [-1.0069],\n",
      "        [-0.7807],\n",
      "        [ 0.5465],\n",
      "        [-3.0280],\n",
      "        [ 1.3413],\n",
      "        [-1.3567],\n",
      "        [ 3.7506],\n",
      "        [ 2.0585],\n",
      "        [-1.4956],\n",
      "        [-0.3084],\n",
      "        [ 3.5954],\n",
      "        [ 1.6332],\n",
      "        [-0.5646],\n",
      "        [-0.8531],\n",
      "        [ 1.4796],\n",
      "        [-0.2825],\n",
      "        [ 6.0667],\n",
      "        [ 2.9826],\n",
      "        [-2.0363],\n",
      "        [ 1.8206],\n",
      "        [ 1.6753],\n",
      "        [ 1.9454],\n",
      "        [ 1.6862],\n",
      "        [-1.6649],\n",
      "        [-0.8192],\n",
      "        [-3.1292],\n",
      "        [ 1.2656],\n",
      "        [-3.3001],\n",
      "        [ 2.9392],\n",
      "        [-0.6255],\n",
      "        [ 0.9287],\n",
      "        [-1.3252],\n",
      "        [ 1.4611],\n",
      "        [ 0.8419],\n",
      "        [-1.8783],\n",
      "        [ 0.7511],\n",
      "        [-0.2545],\n",
      "        [-0.7632],\n",
      "        [ 0.7184],\n",
      "        [-0.3163],\n",
      "        [-1.1048],\n",
      "        [-5.0985],\n",
      "        [ 1.2676],\n",
      "        [-2.5389],\n",
      "        [ 0.8164],\n",
      "        [-2.0115],\n",
      "        [ 1.8793],\n",
      "        [-0.0708],\n",
      "        [-1.3613],\n",
      "        [-2.7219],\n",
      "        [ 1.9667],\n",
      "        [ 1.4092],\n",
      "        [ 1.6623],\n",
      "        [-1.2071],\n",
      "        [ 1.2160],\n",
      "        [ 2.0194],\n",
      "        [ 2.4933],\n",
      "        [ 2.9927],\n",
      "        [-0.8803],\n",
      "        [-0.4213],\n",
      "        [ 1.0374],\n",
      "        [-0.0882],\n",
      "        [-5.5851],\n",
      "        [-3.1400],\n",
      "        [-1.2987],\n",
      "        [ 0.2563],\n",
      "        [-1.8883],\n",
      "        [-0.6565],\n",
      "        [ 0.6891],\n",
      "        [ 3.4478],\n",
      "        [ 0.2704],\n",
      "        [ 0.4090],\n",
      "        [-0.5809],\n",
      "        [-0.5554],\n",
      "        [ 1.2474],\n",
      "        [-1.8870],\n",
      "        [ 0.6974],\n",
      "        [-5.3409],\n",
      "        [-1.3766],\n",
      "        [ 2.1120],\n",
      "        [-1.6531],\n",
      "        [-1.0130],\n",
      "        [ 0.4992],\n",
      "        [ 4.0144],\n",
      "        [-0.0111],\n",
      "        [ 0.8790],\n",
      "        [ 2.7880],\n",
      "        [ 1.6176],\n",
      "        [-1.1641],\n",
      "        [ 2.4917],\n",
      "        [-0.1124],\n",
      "        [ 2.6217],\n",
      "        [ 1.8191],\n",
      "        [-4.6664],\n",
      "        [ 2.4024],\n",
      "        [-2.6183],\n",
      "        [-3.2659],\n",
      "        [ 0.5180],\n",
      "        [-2.1279],\n",
      "        [-1.9879],\n",
      "        [ 1.2835],\n",
      "        [ 2.3098],\n",
      "        [ 3.2572],\n",
      "        [-0.4864],\n",
      "        [ 1.9001],\n",
      "        [-3.1524],\n",
      "        [-0.0331],\n",
      "        [ 2.7975],\n",
      "        [ 2.8965],\n",
      "        [ 2.3061],\n",
      "        [ 1.1343],\n",
      "        [-1.5465],\n",
      "        [ 2.9526],\n",
      "        [-0.9944],\n",
      "        [-3.1238],\n",
      "        [-1.1421],\n",
      "        [-3.6036],\n",
      "        [-0.1114],\n",
      "        [ 1.8045],\n",
      "        [-0.4850],\n",
      "        [-3.0631],\n",
      "        [-1.2470],\n",
      "        [ 0.7333],\n",
      "        [-1.3216],\n",
      "        [ 1.1774],\n",
      "        [ 1.2065],\n",
      "        [-1.2990],\n",
      "        [-3.6353],\n",
      "        [-3.8057],\n",
      "        [ 3.5964],\n",
      "        [ 1.2924],\n",
      "        [-2.4363],\n",
      "        [-0.8931],\n",
      "        [ 1.2566],\n",
      "        [-0.5700],\n",
      "        [ 2.1441],\n",
      "        [ 2.2305],\n",
      "        [-5.0653],\n",
      "        [ 0.6498],\n",
      "        [-1.7291],\n",
      "        [-0.8718],\n",
      "        [ 1.6988],\n",
      "        [-0.9371],\n",
      "        [ 0.7150],\n",
      "        [ 3.4787],\n",
      "        [-0.2296],\n",
      "        [ 1.2556],\n",
      "        [-2.3775],\n",
      "        [-0.4433],\n",
      "        [-1.2915],\n",
      "        [-1.7532],\n",
      "        [ 0.4111],\n",
      "        [-0.9571],\n",
      "        [-1.6437],\n",
      "        [-0.3751],\n",
      "        [ 1.2418],\n",
      "        [-1.8711],\n",
      "        [ 3.7280],\n",
      "        [ 0.2929],\n",
      "        [ 0.6360]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ref_x = float_tensor(r[:, :, None]).to(device)\n",
    "x = float_tensor(x[:, :, None]).to(device)\n",
    "y = float_tensor(y[:, None]).to(device)\n",
    "meta_x = long_tensor(np.arange(ref_x.shape[0]))\n",
    "ref_out_x = encoder(ref_x, meta_x)\n",
    "out_x = encoder(x, meta_x)\n",
    "y_pred, mean_y, logstd_y, _ = decoder.predict(ref_out_x, out_x, sample=False)\n",
    "y_pred, mean_y, logstd_y, _ = corem.predict(mean_y.squeeze(), logstd_y.squeeze(), sample=False)\n",
    "print(y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([258, 68, 1])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
